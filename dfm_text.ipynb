{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c56d9a",
   "metadata": {},
   "source": [
    "## [1] Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "from itertools import chain\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cec10c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    block_size = 1024\n",
    "    n_worker = 12\n",
    "    batch_size = 12\n",
    "\n",
    "    vocab_size = 50257\n",
    "\n",
    "    hidden_size = 768\n",
    "    cond_dim = 128\n",
    "    n_blocks = 10\n",
    "    n_heads = 12\n",
    "    dropout = 0.1\n",
    "\n",
    "    sample_batch_size = 1\n",
    "    num_epochs = 20\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 3e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_model_epochs = 1\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"DFM_v0\"\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8037648",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"Salesforce/wikitext\", name=\"wikitext-103-raw-v1\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "EOS = tokenizer.encode(tokenizer.eos_token)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function re-formats the dataset to make it more human readible\n",
    "def wt_detokenizer(string):\n",
    "    # contractions\n",
    "    string = string.replace(\"s '\", \"s'\")\n",
    "    string = re.sub(r\"/' [0-9]/\", r\"/'[0-9]/\", string)\n",
    "    # number separators\n",
    "    string = string.replace(\" @-@ \", \"-\")\n",
    "    string = string.replace(\" @,@ \", \",\")\n",
    "    string = string.replace(\" @.@ \", \".\")\n",
    "    # punctuation\n",
    "    string = string.replace(\" : \", \": \")\n",
    "    string = string.replace(\" ; \", \"; \")\n",
    "    string = string.replace(\" . \", \". \")\n",
    "    string = string.replace(\" ! \", \"! \")\n",
    "    string = string.replace(\" ? \", \"? \")\n",
    "    string = string.replace(\" , \", \", \")\n",
    "    # double brackets\n",
    "    string = re.sub(r\"\\(\\s*([^\\)]*?)\\s*\\)\", r\"(\\1)\", string)\n",
    "    string = re.sub(r\"\\[\\s*([^\\]]*?)\\s*\\]\", r\"[\\1]\", string)\n",
    "    string = re.sub(r\"{\\s*([^}]*?)\\s*}\", r\"{\\1}\", string)\n",
    "    string = re.sub(r\"\\\"\\s*([^\\\"]*?)\\s*\\\"\", r'\"\\1\"', string)\n",
    "    string = re.sub(r\"'\\s*([^']*?)\\s*'\", r\"'\\1'\", string)\n",
    "    # miscellaneous\n",
    "    string = string.replace(\"= = = =\", \"====\")\n",
    "    string = string.replace(\"= = =\", \"===\")\n",
    "    string = string.replace(\"= =\", \"==\")\n",
    "    string = string.replace(\" \" + chr(176) + \" \", chr(176))\n",
    "    string = string.replace(\" \\n\", \"\\n\")\n",
    "    string = string.replace(\"\\n \", \"\\n\")\n",
    "    string = string.replace(\" N \", \" 1 \")\n",
    "    string = string.replace(\" 's\", \"'s\")\n",
    "    return string\n",
    "\n",
    "# This function preprocesses the dataset and tokenizes the text\n",
    "def preprocess_and_tokenize(example: dict):\n",
    "    text = example[\"text\"]\n",
    "\n",
    "    for i, t in enumerate(text):\n",
    "        text[i] = wt_detokenizer(t)\n",
    "    \n",
    "    tokens = tokenizer(text, return_attention_mask=False)\n",
    "\n",
    "    for token in tokens[\"input_ids\"]:\n",
    "        token.append(EOS)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# This function groups the tokenized texts into blocks of a specified size\n",
    "def group_texts(examples: dict):\n",
    "    block_size = config.block_size\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698225bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = data.map(\n",
    "    preprocess_and_tokenize,\n",
    "    batched=True,\n",
    "    num_proc=config.n_worker,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"text\")\n",
    "\n",
    "chunked_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=config.n_worker,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "chunked_dataset = chunked_dataset.with_format(\"torch\")\n",
    "\n",
    "# Save the processed dataset to disk so to avoid waiting\n",
    "chunked_dataset.save_to_disk(\"chunked_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a444e",
   "metadata": {},
   "source": [
    "## [2] Defining the Discrete Flow Logic and Model\n",
    "\n",
    "To speed up training, we are using Hugging Face Accelerate Library which allows training on multiple GPUs with a simple wrapper. Unfortuantely, for this to work we need to work with .py file so the cell below automatically generates the .py file. We are assuming that the dataset has been pre-precessed according to the instructions in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44685ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile DFM_accelerator.py             #####Uncomment this line#####\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Union\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import DatasetDict, load_from_disk\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "#Following is standard code to set up transformer with adaLN Modulation\n",
    "\n",
    "'''\n",
    "In short the Transformer we define here takes a noisy sample x_t and time t\n",
    "and outputs x_1 (denoised prediction).\n",
    "\n",
    "Here we use fancy Deep Learning Architecture to achieve this but the idea\n",
    "doesn't depend on the architecture.\n",
    "'''\n",
    "class Rotary(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    From: https://github.com/louaaron/Score-Entropy-Discrete-Diffusion\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, base: int = 10_000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self, x: Tensor, seq_dim: int = 1) -> Tuple[Tensor, Tensor]:\n",
    "        seq_len = x.shape[seq_dim]\n",
    "        if seq_len != self.seq_len_cached:\n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq.clone())\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "\n",
    "            # dims are: batch, seq_len, qkv, head, dim\n",
    "            self.cos_cached = emb.cos()[None, :, None, None, :].repeat(1, 1, 3, 1, 1)\n",
    "            self.sin_cached = emb.sin()[None, :, None, None, :].repeat(1, 1, 3, 1, 1)\n",
    "\n",
    "            # This makes the transformation on v an identity.\n",
    "            self.cos_cached[:, :, 2, :, :].fill_(1.0)\n",
    "            self.sin_cached[:, :, 2, :, :].fill_(0.0)\n",
    "\n",
    "        return self.cos_cached, self.sin_cached\n",
    "\n",
    "\n",
    "def rotate_half(x: Tensor) -> Tensor:\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_emb_torch(x, cos, sin, interleaved=False):\n",
    "    \"\"\"\n",
    "    From: https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/layers/rotary.py#L20\n",
    "    \"\"\"\n",
    "    cos = cos[0, :, 0, 0, : cos.shape[-1] // 2]\n",
    "    sin = sin[0, :, 0, 0, : sin.shape[-1] // 2]\n",
    "\n",
    "    ro_dim = cos.shape[-1] * 2\n",
    "    assert ro_dim <= x.shape[-1]\n",
    "    cos = repeat(\n",
    "        cos, \"... d -> ... 1 (2 d)\" if not interleaved else \"... d -> ... 1 (d 2)\"\n",
    "    )\n",
    "    sin = repeat(\n",
    "        sin, \"... d -> ... 1 (2 d)\" if not interleaved else \"... d -> ... 1 (d 2)\"\n",
    "    )\n",
    "\n",
    "    return x[..., :ro_dim] * cos + rotate_half(x[..., :ro_dim]) * sin\n",
    "\n",
    "\n",
    "def bias_dropout_add_scale(\n",
    "        x: Tensor, scale: Tensor, residual: Optional[Tensor],\n",
    "        prob:float, trainning: bool\n",
    ") -> Tensor:\n",
    "    return residual + scale * F.dropout(x, p=prob, training=trainning)\n",
    "\n",
    "def modulate(x: Tensor, shift: Tensor, scale: Tensor) -> Tensor:\n",
    "    return x * (1 + scale) + shift\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size: int,\n",
    "            frequency_embedding_size: int = 256\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.frequency_embedding_size= frequency_embedding_size\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(\n",
    "        time: Tensor,\n",
    "        dim: int,\n",
    "        max_period: int = 10_000\n",
    "    ) -> Tensor:\n",
    "        '''\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        time: 1D Tensor of N indices, one per batch element.\n",
    "        dim:  output dimension\n",
    "        max_period: minimum freq of embedding\n",
    "\n",
    "        return (N, dim)\n",
    "        '''\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period)\n",
    "            * torch.arange(start=0, end=half)\n",
    "            / half\n",
    "        ).to(device=time.device)\n",
    "\n",
    "        args = time[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat(\n",
    "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
    "            )\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, time: Tensor) -> Tensor:\n",
    "        t_freq = self.timestep_embedding(time=time, dim=self.frequency_embedding_size)\n",
    "        t_emb = self.mlp(t_freq)\n",
    "        return t_emb\n",
    "    \n",
    "class DDiTBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            n_heads: int,\n",
    "            cond_dim: int,\n",
    "            mlp_ratio: int = 4,\n",
    "            dropout: float = 0.4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm([self.dim], bias=False)\n",
    "\n",
    "        self.qw = nn.Linear(dim, dim, bias=False)\n",
    "        self.kw = nn.Linear(dim, dim, bias=False)\n",
    "        self.vw = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "        self.attn_out = nn.Linear(dim, dim, bias=False)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm([self.dim], bias=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_ratio*dim, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_ratio*dim, dim, bias=False)\n",
    "        )\n",
    "\n",
    "        self.adaLN_modulation = nn.Linear(cond_dim, 6*dim, bias=True)\n",
    "        self.adaLN_modulation.weight.data.zero_()\n",
    "        self.adaLN_modulation.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: Tensor, rotary_cos_sin: Tensor, c: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "\n",
    "        (\n",
    "            shift_msa,\n",
    "            scale_msa,\n",
    "            gate_msa,\n",
    "            shift_mlp,\n",
    "            scale_mlp,\n",
    "            gate_mlp,\n",
    "        ) = self.adaLN_modulation(c)[:, None].chunk(6, dim=2)\n",
    "\n",
    "        x_skip = x\n",
    "        x = modulate(x=self.norm1(x), shift=shift_msa, scale=scale_msa)\n",
    "\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "\n",
    "        q, k, v = (\n",
    "            item.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "            for item in (q, k, v)\n",
    "        )\n",
    "\n",
    "        cos, sin = rotary_cos_sin\n",
    "        original_dtype = q.dtype\n",
    "\n",
    "        q = apply_rotary_emb_torch(\n",
    "            x=q.float(), cos=cos.float(), sin=sin.float()\n",
    "        ).to(original_dtype)\n",
    "\n",
    "        k = apply_rotary_emb_torch(\n",
    "            x=k.float(), cos=cos.float(), sin=sin.float()\n",
    "        ).to(original_dtype)\n",
    "\n",
    "        q, k, v = (item.transpose(1, 2) for item in (q, k, v))\n",
    "\n",
    "        x = F.scaled_dot_product_attention(query=q, key=k, value=v)\n",
    "        x = rearrange(x, \"b h s d -> b s (h d)\", b=batch_size)\n",
    "        x = bias_dropout_add_scale(\n",
    "            x = self.attn_out(x),\n",
    "            scale=gate_msa,\n",
    "            residual=x_skip,\n",
    "            prob=self.dropout,\n",
    "            trainning=self.training\n",
    "        )\n",
    "\n",
    "        skip_x = modulate(x=self.norm2(x), shift=shift_mlp, scale=scale_mlp)\n",
    "\n",
    "        x = bias_dropout_add_scale(\n",
    "            x = self.mlp(skip_x),\n",
    "            scale=gate_mlp,\n",
    "            residual=skip_x,\n",
    "            prob=self.dropout,\n",
    "            trainning=self.training,\n",
    "        )\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DDitFinalLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size: int,\n",
    "            out_channels: int,\n",
    "            cond_dim: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, out_channels)\n",
    "        self.linear.weight.data.zero_()\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "        self.adaLN_modulation = nn.Linear(cond_dim, 2*hidden_size, bias=True)\n",
    "        self.adaLN_modulation.weight.data.zero_()\n",
    "        self.adaLN_modulation.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: Tensor, c: Tensor) -> Tensor:\n",
    "        shift, scale = self.adaLN_modulation(c)[:, None].chunk(2, dim=2)\n",
    "        x = modulate(x=self.norm_final(x), shift=shift, scale=scale)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            masked: bool,\n",
    "            config,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.vocab_embed = nn.Embedding(self.vocab_size, config.hidden_size)\n",
    "\n",
    "        self.time_embedding = TimestepEmbedder(hidden_size=config.cond_dim)\n",
    "        self.rotary_emb = Rotary(dim=config.hidden_size // config.n_heads)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                DDiTBlock(\n",
    "                    dim=config.hidden_size,\n",
    "                    n_heads=config.n_heads,\n",
    "                    cond_dim=config.cond_dim,\n",
    "                    dropout=config.dropout,\n",
    "                )\n",
    "                for _ in range(config.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.output_layer = DDitFinalLayer(\n",
    "            hidden_size=config.hidden_size,\n",
    "            out_channels=vocab_size,\n",
    "            cond_dim=config.cond_dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t: Tensor, time: Tensor) -> Tensor:\n",
    "        x = self.vocab_embed(x_t)\n",
    "        c = F.silu(self.time_embedding(time=time))\n",
    "\n",
    "        rotary_cos_sin = self.rotary_emb(x=x)\n",
    "\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x=x, rotary_cos_sin=rotary_cos_sin, c=c)\n",
    "\n",
    "        x = self.output_layer(x=x, c=c)\n",
    "\n",
    "        return x\n",
    "    \n",
    "#Here we define the MAIN LOGIC for Discrete Flow Matching\n",
    "@dataclass\n",
    "class SchedulerOutput:\n",
    "    alpha_t: Tensor\n",
    "    sigma_t: Tensor\n",
    "\n",
    "    d_alpha_t: Tensor\n",
    "    d_sigma_t: Tensor\n",
    "\n",
    "class Scheduler:\n",
    "    def __call__(self, t: Tensor) -> SchedulerOutput:\n",
    "        ...\n",
    "\n",
    "class PolynomialConvexScheduler(Scheduler):\n",
    "    def __init__(self, n: Union[float, int]) -> None:\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, t: Tensor) -> SchedulerOutput:\n",
    "        return SchedulerOutput(\n",
    "            alpha_t=t**self.n,\n",
    "            sigma_t=1 - t**self.n,\n",
    "            d_alpha_t=self.n * (t**(self.n - 1)),\n",
    "            d_sigma_t=-self.n * (t**(self.n - 1))\n",
    "        )\n",
    "    \n",
    "@dataclass\n",
    "class DiscretePathSample:\n",
    "    x_1: Tensor\n",
    "    x_0: Tensor\n",
    "    t:   Tensor\n",
    "    x_t: Tensor\n",
    "\n",
    "class ProbPath:\n",
    "    def sample(self, x_0: Tensor, x_1: Tensor, t: Tensor) -> DiscretePathSample:\n",
    "        ...\n",
    "\n",
    "    def assert_sample_shape(self, x_0: Tensor, x_1: Tensor, t: Tensor):\n",
    "        assert t.ndim == 1, \"t must have shape [batch_size]\"\n",
    "        assert t.shape[0] == x_0.shape[0] == x_1.shape[0], \"Mismatch Batch Size\"\n",
    "\n",
    "class MixtureDiscreteProbPath(ProbPath):\n",
    "    def __init__(self, scheduler: Scheduler) -> None:\n",
    "        super().__init__()\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def sample(self, x_0: Tensor, x_1: Tensor, t: Tensor) -> DiscretePathSample:\n",
    "        self.assert_sample_shape(x_0=x_0, x_1=x_1, t=t)\n",
    "\n",
    "        B, L = x_1.shape\n",
    "\n",
    "        sigma_t = self.scheduler(t).sigma_t\n",
    "        sigma_t = rearrange(sigma_t, 'd -> d 1')\n",
    "        sigma_t = sigma_t.expand_as(x_1)\n",
    "\n",
    "        source_indices = torch.rand(size=x_1.shape, device=x_1.device) < sigma_t\n",
    "        x_t = torch.where(condition=source_indices, input=x_0, other=x_1)\n",
    "\n",
    "        return DiscretePathSample(\n",
    "            x_t = x_t,\n",
    "            x_1 = x_1,\n",
    "            x_0 = x_0,\n",
    "            t = t,\n",
    "        )\n",
    "    \n",
    "class UniformSourceDistribution:\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def sample(self, tensor_size: Tuple[int, ...], device: torch.device) -> Tensor:\n",
    "        return torch.randint(size=tensor_size, high=self.vocab_size, device=device)\n",
    "\n",
    "    def sample_like(self, tensor_like: Tensor) -> Tensor:\n",
    "        return torch.randint_like(tensor_like, high=self.vocab_size)\n",
    "    \n",
    "\n",
    "#####################Training Config######################\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    block_size = 1024\n",
    "    n_worker = 1\n",
    "    batch_size = 12\n",
    "\n",
    "    vocab_size = 50257\n",
    "\n",
    "    hidden_size = 768\n",
    "    cond_dim = 128\n",
    "    n_blocks = 10\n",
    "    n_heads = 12\n",
    "    dropout = 0.1\n",
    "\n",
    "    num_epochs = 20\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 3e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_model_epochs = 1\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"DFM_v0\"\n",
    "\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "\n",
    "def train_loop(\n",
    "        config: TrainingConfig,\n",
    "        source: UniformSourceDistribution,\n",
    "        velocity: MixtureDiscreteProbPath, \n",
    "        train_dataloader: DatasetDict,\n",
    "        ):\n",
    "\n",
    "    model = Transformer(\n",
    "    config=config,\n",
    "    vocab_size=config.vocab_size,\n",
    "    masked=False\n",
    "    )\n",
    "\n",
    "    optim = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = config.learning_rate\n",
    "    )\n",
    "\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optim,\n",
    "        num_warmup_steps=config.lr_warmup_steps,\n",
    "        num_training_steps=(len(train_dataloader) * config.num_epochs)\n",
    "    )\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_examples\")\n",
    "\n",
    "    model, source, velocity, optim, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, source, velocity, optim, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, x_1 in enumerate(train_dataloader):\n",
    "            x_1 = x_1[\"input_ids\"].to(accelerator.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x_0 = source.sample_like(x_1)\n",
    "                t = torch.rand(x_1.shape[0], device=accelerator.device) * (1 - 1e-3)\n",
    "                vel_t = velocity.sample(t=t, x_1=x_1, x_0=x_0)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                logits = model(x_t=vel_t.x_t, time=vel_t.t)\n",
    "                loss = F.cross_entropy(logits.flatten(0, 1), x_1.flatten(0, 1), reduction=\"mean\")\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optim.step()\n",
    "                lr_scheduler.step()\n",
    "                optim.zero_grad()\n",
    "        \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step+=1\n",
    "    \n",
    "        if accelerator.is_main_process:\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                accelerator.save_model(model, config.output_dir)\n",
    "\n",
    "def main():\n",
    "    config = TrainingConfig()\n",
    "\n",
    "    vocab_size = config.vocab_size\n",
    "    source_distribution = UniformSourceDistribution(vocab_size=vocab_size)\n",
    "    scheduler = PolynomialConvexScheduler(n=2)\n",
    "    path = MixtureDiscreteProbPath(scheduler=scheduler)\n",
    "\n",
    "    chunked_dataset = load_from_disk(\"chunked_dataset\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "            chunked_dataset[\"train\"],\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=config.n_worker\n",
    "        )\n",
    "\n",
    "    train_loop(config, source_distribution, path, train_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Before we run DFMaccelerator.py, we need to configure accelerate library.\n",
    "This make sure we are optimally using our hardware.\n",
    "'''\n",
    "\n",
    "!accelerate config default\n",
    "\n",
    "!cat path/to/your/default_config.yaml #Check if the config is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's run the training script\n",
    "\n",
    "!accelerate launch DFM_accelerator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f4db7",
   "metadata": {},
   "source": [
    "## [3] Inference & Benchmarking\n",
    "\n",
    "Here we will load the model and perform conditional / unconditional generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "904f3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7218eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    block_size = 1024\n",
    "    n_worker = 1\n",
    "    batch_size = 12\n",
    "\n",
    "    vocab_size = 50257\n",
    "\n",
    "    hidden_size = 768\n",
    "    cond_dim = 128\n",
    "    n_blocks = 10\n",
    "    n_heads = 12\n",
    "    dropout = 0.1\n",
    "\n",
    "    # train_batch_size = 8\n",
    "    # eval_batch_size = 16\n",
    "    sample_batch_size = 1\n",
    "    num_epochs = 20\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 3e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_model_epochs = 1\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"DFM_v0\"\n",
    "\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce3825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training, accelerate was checkpointing the model\n",
    "# Here we create a dummy model to load the checkpoint\n",
    "\n",
    "class Rotary(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    From: https://github.com/louaaron/Score-Entropy-Discrete-Diffusion\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, base: int = 10_000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self, x: Tensor, seq_dim: int = 1) -> Tuple[Tensor, Tensor]:\n",
    "        seq_len = x.shape[seq_dim]\n",
    "        if seq_len != self.seq_len_cached:\n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq.clone())\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "\n",
    "            # dims are: batch, seq_len, qkv, head, dim\n",
    "            self.cos_cached = emb.cos()[None, :, None, None, :].repeat(1, 1, 3, 1, 1)\n",
    "            self.sin_cached = emb.sin()[None, :, None, None, :].repeat(1, 1, 3, 1, 1)\n",
    "\n",
    "            # This makes the transformation on v an identity.\n",
    "            self.cos_cached[:, :, 2, :, :].fill_(1.0)\n",
    "            self.sin_cached[:, :, 2, :, :].fill_(0.0)\n",
    "\n",
    "        return self.cos_cached, self.sin_cached\n",
    "\n",
    "\n",
    "def rotate_half(x: Tensor) -> Tensor:\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_emb_torch(x, cos, sin, interleaved=False):\n",
    "    \"\"\"\n",
    "    From: https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/layers/rotary.py#L20\n",
    "    \"\"\"\n",
    "    cos = cos[0, :, 0, 0, : cos.shape[-1] // 2]\n",
    "    sin = sin[0, :, 0, 0, : sin.shape[-1] // 2]\n",
    "\n",
    "    ro_dim = cos.shape[-1] * 2\n",
    "    assert ro_dim <= x.shape[-1]\n",
    "    cos = repeat(\n",
    "        cos, \"... d -> ... 1 (2 d)\" if not interleaved else \"... d -> ... 1 (d 2)\"\n",
    "    )\n",
    "    sin = repeat(\n",
    "        sin, \"... d -> ... 1 (2 d)\" if not interleaved else \"... d -> ... 1 (d 2)\"\n",
    "    )\n",
    "\n",
    "    return x[..., :ro_dim] * cos + rotate_half(x[..., :ro_dim]) * sin\n",
    "\n",
    "\n",
    "def bias_dropout_add_scale(\n",
    "        x: Tensor, scale: Tensor, residual: Optional[Tensor],\n",
    "        prob:float, trainning: bool\n",
    ") -> Tensor:\n",
    "    return residual + scale * F.dropout(x, p=prob, training=trainning)\n",
    "\n",
    "def modulate(x: Tensor, shift: Tensor, scale: Tensor) -> Tensor:\n",
    "    return x * (1 + scale) + shift\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size: int,\n",
    "            frequency_embedding_size: int = 256\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.frequency_embedding_size= frequency_embedding_size\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(\n",
    "        time: Tensor,\n",
    "        dim: int,\n",
    "        max_period: int = 10_000\n",
    "    ) -> Tensor:\n",
    "        '''\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        time: 1D Tensor of N indices, one per batch element.\n",
    "        dim:  output dimension\n",
    "        max_period: minimum freq of embedding\n",
    "\n",
    "        return (N, dim)\n",
    "        '''\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period)\n",
    "            * torch.arange(start=0, end=half)\n",
    "            / half\n",
    "        ).to(device=time.device)\n",
    "\n",
    "        args = time[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat(\n",
    "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
    "            )\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, time: Tensor) -> Tensor:\n",
    "        t_freq = self.timestep_embedding(time=time, dim=self.frequency_embedding_size)\n",
    "        t_emb = self.mlp(t_freq)\n",
    "        return t_emb\n",
    "    \n",
    "class DDiTBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            n_heads: int,\n",
    "            cond_dim: int,\n",
    "            mlp_ratio: int = 4,\n",
    "            dropout: float = 0.4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm([self.dim], bias=False)\n",
    "\n",
    "        self.qw = nn.Linear(dim, dim, bias=False)\n",
    "        self.kw = nn.Linear(dim, dim, bias=False)\n",
    "        self.vw = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "        self.attn_out = nn.Linear(dim, dim, bias=False)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm([self.dim], bias=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_ratio*dim, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_ratio*dim, dim, bias=False)\n",
    "        )\n",
    "\n",
    "        self.adaLN_modulation = nn.Linear(cond_dim, 6*dim, bias=True)\n",
    "        self.adaLN_modulation.weight.data.zero_()\n",
    "        self.adaLN_modulation.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: Tensor, rotary_cos_sin: Tensor, c: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "\n",
    "        (\n",
    "            shift_msa,\n",
    "            scale_msa,\n",
    "            gate_msa,\n",
    "            shift_mlp,\n",
    "            scale_mlp,\n",
    "            gate_mlp,\n",
    "        ) = self.adaLN_modulation(c)[:, None].chunk(6, dim=2)\n",
    "\n",
    "        x_skip = x\n",
    "        x = modulate(x=self.norm1(x), shift=shift_msa, scale=scale_msa)\n",
    "\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "\n",
    "        q, k, v = (\n",
    "            item.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "            for item in (q, k, v)\n",
    "        )\n",
    "\n",
    "        cos, sin = rotary_cos_sin\n",
    "        original_dtype = q.dtype\n",
    "\n",
    "        q = apply_rotary_emb_torch(\n",
    "            x=q.float(), cos=cos.float(), sin=sin.float()\n",
    "        ).to(original_dtype)\n",
    "\n",
    "        k = apply_rotary_emb_torch(\n",
    "            x=k.float(), cos=cos.float(), sin=sin.float()\n",
    "        ).to(original_dtype)\n",
    "\n",
    "        q, k, v = (item.transpose(1, 2) for item in (q, k, v))\n",
    "\n",
    "        x = F.scaled_dot_product_attention(query=q, key=k, value=v)\n",
    "        x = rearrange(x, \"b h s d -> b s (h d)\", b=batch_size)\n",
    "        x = bias_dropout_add_scale(\n",
    "            x = self.attn_out(x),\n",
    "            scale=gate_msa,\n",
    "            residual=x_skip,\n",
    "            prob=self.dropout,\n",
    "            trainning=self.training\n",
    "        )\n",
    "\n",
    "        skip_x = modulate(x=self.norm2(x), shift=shift_mlp, scale=scale_mlp)\n",
    "\n",
    "        x = bias_dropout_add_scale(\n",
    "            x = self.mlp(skip_x),\n",
    "            scale=gate_mlp,\n",
    "            residual=skip_x,\n",
    "            prob=self.dropout,\n",
    "            trainning=self.training,\n",
    "        )\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DDitFinalLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size: int,\n",
    "            out_channels: int,\n",
    "            cond_dim: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, out_channels)\n",
    "        self.linear.weight.data.zero_()\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "        self.adaLN_modulation = nn.Linear(cond_dim, 2*hidden_size, bias=True)\n",
    "        self.adaLN_modulation.weight.data.zero_()\n",
    "        self.adaLN_modulation.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: Tensor, c: Tensor) -> Tensor:\n",
    "        shift, scale = self.adaLN_modulation(c)[:, None].chunk(2, dim=2)\n",
    "        x = modulate(x=self.norm_final(x), shift=shift, scale=scale)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            masked: bool,\n",
    "            config,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.vocab_embed = nn.Embedding(self.vocab_size, config.hidden_size)\n",
    "\n",
    "        self.time_embedding = TimestepEmbedder(hidden_size=config.cond_dim)\n",
    "        self.rotary_emb = Rotary(dim=config.hidden_size // config.n_heads)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                DDiTBlock(\n",
    "                    dim=config.hidden_size,\n",
    "                    n_heads=config.n_heads,\n",
    "                    cond_dim=config.cond_dim,\n",
    "                    dropout=config.dropout,\n",
    "                )\n",
    "                for _ in range(config.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.output_layer = DDitFinalLayer(\n",
    "            hidden_size=config.hidden_size,\n",
    "            out_channels=vocab_size,\n",
    "            cond_dim=config.cond_dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t: Tensor, time: Tensor) -> Tensor:\n",
    "        x = self.vocab_embed(x_t)\n",
    "        c = F.silu(self.time_embedding(time=time))\n",
    "\n",
    "        rotary_cos_sin = self.rotary_emb(x=x)\n",
    "\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x=x, rotary_cos_sin=rotary_cos_sin, c=c)\n",
    "\n",
    "        x = self.output_layer(x=x, c=c)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7df68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    config=config,\n",
    "    vocab_size=config.vocab_size,\n",
    "    masked=False\n",
    "    )\n",
    "\n",
    "state_dict = load_file(\"path/to/model.safetensors\", device=\"cpu\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "260d70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SchedulerOutput:\n",
    "    alpha_t: Tensor\n",
    "    sigma_t: Tensor\n",
    "\n",
    "    d_alpha_t: Tensor\n",
    "    d_sigma_t: Tensor\n",
    "\n",
    "class Scheduler:\n",
    "    def __call__(self, t: Tensor) -> SchedulerOutput:\n",
    "        ...\n",
    "\n",
    "class PolynomialConvexScheduler(Scheduler):\n",
    "    def __init__(self, n: Union[float, int]) -> None:\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, t: Tensor) -> SchedulerOutput:\n",
    "        return SchedulerOutput(\n",
    "            alpha_t=t**self.n,\n",
    "            sigma_t=1 - t**self.n,\n",
    "            d_alpha_t=self.n * (t**(self.n - 1)),\n",
    "            d_sigma_t=-self.n * (t**(self.n - 1))\n",
    "        )\n",
    "    \n",
    "@dataclass\n",
    "class DiscretePathSample:\n",
    "    x_1: Tensor\n",
    "    x_0: Tensor\n",
    "    t:   Tensor\n",
    "    x_t: Tensor\n",
    "\n",
    "class ProbPath:\n",
    "    def sample(self, x_0: Tensor, x_1: Tensor, t: Tensor) -> DiscretePathSample:\n",
    "        ...\n",
    "\n",
    "    def assert_sample_shape(self, x_0: Tensor, x_1: Tensor, t: Tensor):\n",
    "        assert t.ndim == 1, \"t must have shape [batch_size]\"\n",
    "        assert t.shape[0] == x_0.shape[0] == x_1.shape[0], \"Mismatch Batch Size\"\n",
    "\n",
    "class MixtureDiscreteProbPath(ProbPath):\n",
    "    def __init__(self, scheduler: Scheduler) -> None:\n",
    "        super().__init__()\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def sample(self, x_0: Tensor, x_1: Tensor, t: Tensor) -> DiscretePathSample:\n",
    "        self.assert_sample_shape(x_0=x_0, x_1=x_1, t=t)\n",
    "\n",
    "        B, L = x_1.shape\n",
    "\n",
    "        sigma_t = self.scheduler(t).sigma_t\n",
    "        sigma_t = rearrange(sigma_t, 'd -> d 1')\n",
    "        sigma_t = sigma_t.expand_as(x_1)\n",
    "\n",
    "        source_indices = torch.rand(size=x_1.shape, device=x_1.device) < sigma_t\n",
    "        x_t = torch.where(condition=source_indices, input=x_0, other=x_1)\n",
    "\n",
    "        return DiscretePathSample(\n",
    "            x_t = x_t,\n",
    "            x_1 = x_1,\n",
    "            x_0 = x_0,\n",
    "            t = t,\n",
    "        )\n",
    "\n",
    "class UniformSourceDistribution:\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def sample(self, tensor_size: Tuple[int, ...], device: torch.device) -> Tensor:\n",
    "        return torch.randint(size=tensor_size, high=self.vocab_size, device=device)\n",
    "\n",
    "    def sample_like(self, tensor_like: Tensor) -> Tensor:\n",
    "        return torch.randint_like(tensor_like, high=self.vocab_size)\n",
    "    \n",
    "class MixtureDiscreteEulerSolver:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            path: MixtureDiscreteProbPath,\n",
    "            vocabulary_size: int,\n",
    "            source_distribution_p: Optional[Tensor] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.source_distribution_p = source_distribution_p\n",
    "\n",
    "    def sample(\n",
    "            self,\n",
    "            x_init: Tensor,\n",
    "            step_size: float,\n",
    "            cond_text: Optional[Tensor],\n",
    "            **model_extras,\n",
    "    )-> Tensor:\n",
    "        steps_counter = 0\n",
    "\n",
    "        x_t = x_init.clone()\n",
    "        t = torch.Tensor([0]).to(x_init.device)\n",
    "        # res = [(x_t, t)]\n",
    "\n",
    "        if cond_text is not None:\n",
    "            cond_text = cond_text.to(x_init.device)\n",
    "            x_t[:, :len(cond_text)] = cond_text\n",
    "\n",
    "\n",
    "        ctx = tqdm(total=1.0, desc=f\"NFE: {steps_counter}\")\n",
    "        \n",
    "        with ctx:\n",
    "            while t < 1.0 - 1e-3:\n",
    "                p_1t = self.model(x_t, t.repeat(x_t.shape[0]))\n",
    "                h = min(step_size, 1.0 - t.item())\n",
    "\n",
    "                scheduler_output = self.path.scheduler(t=t)\n",
    "\n",
    "                k_t = scheduler_output.alpha_t\n",
    "                d_k_t = scheduler_output.d_alpha_t\n",
    "\n",
    "                one_hot_x_t = F.one_hot(x_t, num_classes=self.vocabulary_size).float()\n",
    "\n",
    "                u = (p_1t - one_hot_x_t) * ((d_k_t)/(1 - k_t)) \n",
    "                x_t = torch.distributions.Categorical(probs=(one_hot_x_t + h*u)).sample()\n",
    "                \n",
    "                if cond_text is not None:\n",
    "                    x_t[:, :len(cond_text)] = cond_text\n",
    "                \n",
    "                t = t + h\n",
    "                # res.append((x_t, t))\n",
    "                steps_counter += 1\n",
    "\n",
    "                ctx.n = t.item()\n",
    "                ctx.refresh()\n",
    "                ctx.set_description(f\"NFE: {steps_counter}\")\n",
    "\n",
    "        return x_t\n",
    "    \n",
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor, **extras) -> Tensor:\n",
    "        # Note: logit's precision is important.\n",
    "        return torch.softmax(self.model(x_t=x, time=t, **extras).float(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2434e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(\n",
    "        wrapped_probability_denoiser: WrappedModel,\n",
    "        vocab_size: int,\n",
    "        tokenizer: GPT2TokenizerFast,\n",
    "        rank: int,\n",
    "        device: torch.device,\n",
    "        path: ProbPath,\n",
    "        source_dist: UniformSourceDistribution,\n",
    "        cond_text: Tensor,\n",
    "        sample_batch_size: int,\n",
    "        sequence_length: int,\n",
    "        sampling_steps: int,\n",
    "        sample_dir: Optional[Path] = None,\n",
    "):\n",
    "    \n",
    "    solver = MixtureDiscreteEulerSolver(\n",
    "        model=wrapped_probability_denoiser,\n",
    "        path=path,\n",
    "        vocabulary_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    x_init = source_dist.sample(\n",
    "        tensor_size=(sample_batch_size, sequence_length), device=device\n",
    "    )\n",
    "\n",
    "    sample_hist = solver.sample(\n",
    "        x_init=x_init,\n",
    "        step_size=1 / sampling_steps,\n",
    "        cond_text=cond_text\n",
    "    )\n",
    "\n",
    "    return tokenizer.batch_decode(sample_hist.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f58f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "wrapped_model = WrappedModel(model).to(device).eval()\n",
    "\n",
    "vocab_size = config.vocab_size\n",
    "source_distribution = UniformSourceDistribution(vocab_size=vocab_size)\n",
    "scheduler = PolynomialConvexScheduler(n=2)\n",
    "path = MixtureDiscreteProbPath(scheduler=scheduler)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9550714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_text = \"Princess Peach uses a letter to invite Mario to come to her castle for a cake she has baked for him. When he arrives, Mario discovers that Bowser has invaded the castle and\"\n",
    "cond_text = torch.Tensor(tokenizer(cond_text)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46403f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ba111d8bda45da8007ea43376206f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NFE: 0:   0%|          | 0/1.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config.sample_batch_size = 1\n",
    "\n",
    "sample_hist = generate_samples(\n",
    "    wrapped_probability_denoiser=wrapped_model,\n",
    "    vocab_size=config.vocab_size,\n",
    "    tokenizer=tokenizer,\n",
    "    rank=0,\n",
    "    device=device,\n",
    "    path=path,\n",
    "    source_dist=source_distribution,\n",
    "    cond_text=cond_text,\n",
    "    sample_batch_size=config.sample_batch_size,\n",
    "    sequence_length=150,\n",
    "    sampling_steps=1024,\n",
    "    sample_dir=config.output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "144ae26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Princess Peach uses a letter to invite Mario to come to her castle for a cake she has baked for him. When he arrives, Mario discovers that Bowser has invaded the castle and become trapped in the land scare Cousins, leaving the castle to let him remove herself. However, his lord would act too swiftly to drain water and allows Mario to successfully destroy Maurice. Outside the castle, Mario regains power to retake the castle, which he causes sewers to increase his cost. Rod Tim Owing to Coventry perceived that Nigel a \"ombensive condition\" and fears that he could not make life in the castle but bring Nigel back to the dinner party.\\n<|endoftext|> The castle begins and toss them onto the castle. Continuing south, the castle to exit']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
